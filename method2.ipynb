{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Role</th>\n",
       "      <th>User Story</th>\n",
       "      <th>Verbs</th>\n",
       "      <th>Adjectives</th>\n",
       "      <th>Nouns</th>\n",
       "      <th>Imp Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Data user</td>\n",
       "      <td>I want to have the 12-19-2017 deletions proce...</td>\n",
       "      <td>['processed']</td>\n",
       "      <td>['12-19-2017']</td>\n",
       "      <td>['deletions']</td>\n",
       "      <td>['want', '12-19-2017', 'deletions', 'processed']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>UI designer</td>\n",
       "      <td>I want to redesign the Resources page, so tha...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['redesign', 'Resources', 'matches', 'Broker',...</td>\n",
       "      <td>['want', 'redesign', 'Resources', 'matches', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>UI designer</td>\n",
       "      <td>I want to report to the Agencies about user t...</td>\n",
       "      <td>['making']</td>\n",
       "      <td>['aware', 'better']</td>\n",
       "      <td>['report', 'Agencies', 'user', 'testing', 'con...</td>\n",
       "      <td>['want', 'report', 'Agencies', 'user', 'testin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>UI designer</td>\n",
       "      <td>I want to move on to round 2 of DABS or FABS ...</td>\n",
       "      <td>['landing']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['move', 'DABS', 'FABS', 'edits', 'approvals',...</td>\n",
       "      <td>['want', 'move', 'DABS', 'FABS', 'landing', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>UI designer</td>\n",
       "      <td>I want to move on to round 2 of Homepage edit...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['move', 'edits', 'approvals', 'leadership']</td>\n",
       "      <td>['want', 'move', 'edits', 'get', 'approvals', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>1661</td>\n",
       "      <td>1661</td>\n",
       "      <td>admin</td>\n",
       "      <td>I want to know when Zoonibot should give an e...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Zoonibot', 'explanation']</td>\n",
       "      <td>['want', 'know', 'Zoonibot', 'explanation']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>1662</td>\n",
       "      <td>1662</td>\n",
       "      <td>admin</td>\n",
       "      <td>I want to know what Zoonibot should say to a ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['Zoonibot', 'volunteer']</td>\n",
       "      <td>['want', 'know', 'Zoonibot', 'volunteer']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>1663</td>\n",
       "      <td>1663</td>\n",
       "      <td>admin</td>\n",
       "      <td>I want to group subjects by similarity.</td>\n",
       "      <td>['subjects']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['group', 'similarity']</td>\n",
       "      <td>['want', 'group', 'subjects', 'similarity']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>1664</td>\n",
       "      <td>1664</td>\n",
       "      <td>admin</td>\n",
       "      <td>I want to recommend different projects to vol...</td>\n",
       "      <td>['based']</td>\n",
       "      <td>['recommend', 'previous']</td>\n",
       "      <td>['projects', 'volunteers', 'experiences']</td>\n",
       "      <td>['want', 'recommend', 'projects', 'volunteers'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>1665</td>\n",
       "      <td>1665</td>\n",
       "      <td>admin</td>\n",
       "      <td>I want to see a summary of articles, so that ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['summary']</td>\n",
       "      <td>['articles', 'reuse']</td>\n",
       "      <td>['want', 'summary', 'articles', 'reuse']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1666 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0.1  Unnamed: 0         Role  \\\n",
       "0                0           0    Data user   \n",
       "1                1           1  UI designer   \n",
       "2                2           2  UI designer   \n",
       "3                3           3  UI designer   \n",
       "4                4           4  UI designer   \n",
       "...            ...         ...          ...   \n",
       "1661          1661        1661        admin   \n",
       "1662          1662        1662        admin   \n",
       "1663          1663        1663        admin   \n",
       "1664          1664        1664        admin   \n",
       "1665          1665        1665        admin   \n",
       "\n",
       "                                             User Story          Verbs  \\\n",
       "0      I want to have the 12-19-2017 deletions proce...  ['processed']   \n",
       "1      I want to redesign the Resources page, so tha...             []   \n",
       "2      I want to report to the Agencies about user t...     ['making']   \n",
       "3      I want to move on to round 2 of DABS or FABS ...    ['landing']   \n",
       "4      I want to move on to round 2 of Homepage edit...             []   \n",
       "...                                                 ...            ...   \n",
       "1661   I want to know when Zoonibot should give an e...             []   \n",
       "1662   I want to know what Zoonibot should say to a ...             []   \n",
       "1663            I want to group subjects by similarity.   ['subjects']   \n",
       "1664   I want to recommend different projects to vol...      ['based']   \n",
       "1665   I want to see a summary of articles, so that ...             []   \n",
       "\n",
       "                     Adjectives  \\\n",
       "0                ['12-19-2017']   \n",
       "1                            []   \n",
       "2           ['aware', 'better']   \n",
       "3                            []   \n",
       "4                            []   \n",
       "...                         ...   \n",
       "1661                         []   \n",
       "1662                         []   \n",
       "1663                         []   \n",
       "1664  ['recommend', 'previous']   \n",
       "1665                ['summary']   \n",
       "\n",
       "                                                  Nouns  \\\n",
       "0                                         ['deletions']   \n",
       "1     ['redesign', 'Resources', 'matches', 'Broker',...   \n",
       "2     ['report', 'Agencies', 'user', 'testing', 'con...   \n",
       "3     ['move', 'DABS', 'FABS', 'edits', 'approvals',...   \n",
       "4          ['move', 'edits', 'approvals', 'leadership']   \n",
       "...                                                 ...   \n",
       "1661                        ['Zoonibot', 'explanation']   \n",
       "1662                          ['Zoonibot', 'volunteer']   \n",
       "1663                            ['group', 'similarity']   \n",
       "1664          ['projects', 'volunteers', 'experiences']   \n",
       "1665                              ['articles', 'reuse']   \n",
       "\n",
       "                                              Imp Words  \n",
       "0      ['want', '12-19-2017', 'deletions', 'processed']  \n",
       "1     ['want', 'redesign', 'Resources', 'matches', '...  \n",
       "2     ['want', 'report', 'Agencies', 'user', 'testin...  \n",
       "3     ['want', 'move', 'DABS', 'FABS', 'landing', 'e...  \n",
       "4     ['want', 'move', 'edits', 'get', 'approvals', ...  \n",
       "...                                                 ...  \n",
       "1661        ['want', 'know', 'Zoonibot', 'explanation']  \n",
       "1662          ['want', 'know', 'Zoonibot', 'volunteer']  \n",
       "1663        ['want', 'group', 'subjects', 'similarity']  \n",
       "1664  ['want', 'recommend', 'projects', 'volunteers'...  \n",
       "1665           ['want', 'summary', 'articles', 'reuse']  \n",
       "\n",
       "[1666 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"train.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a Column User story from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        I want to have the 12-19-2017 deletions proce...\n",
       "1        I want to redesign the Resources page, so tha...\n",
       "2        I want to report to the Agencies about user t...\n",
       "3        I want to move on to round 2 of DABS or FABS ...\n",
       "4        I want to move on to round 2 of Homepage edit...\n",
       "                              ...                        \n",
       "1661     I want to know when Zoonibot should give an e...\n",
       "1662     I want to know what Zoonibot should say to a ...\n",
       "1663              I want to group subjects by similarity.\n",
       "1664     I want to recommend different projects to vol...\n",
       "1665     I want to see a summary of articles, so that ...\n",
       "Name: User Story, Length: 1666, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = df['User Story']\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the custom stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        stopwords_list = file.read().splitlines()\n",
    "    return stopwords_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize and Split to verbs, adjectives,noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        stopwords_list = file.read().splitlines()\n",
    "    return stopwords_list\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    stopwords_list = load_stop(\"./stopwords-en.txt\")\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stopwords_list]\n",
    "    return filtered_tokens\n",
    "\n",
    "def identify_pos(tokens):\n",
    "    # Tag tokens with parts of speech\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # verbs = [word for word, tag in pos_tags if tag.startswith('VB')]\n",
    "    # adjectives = [word for word, tag in pos_tags if tag.startswith('JJ')]\n",
    "    # nouns = [word for word, tag in pos_tags if tag.startswith('NN')]\n",
    "    # pos_list = verbs + adjectives + nouns\n",
    "    pos_list = []\n",
    "    \n",
    "    for word, tag in pos_tags:\n",
    "        if tag.startswith('VB') or tag.startswith('JJ') or tag.startswith('NN'):\n",
    "            pos_list.append(word)\n",
    "    return pos_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing each Row in Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    text = row['User Story']\n",
    "    filtered_tokens = custom_tokenizer(text)\n",
    "    words = identify_pos(filtered_tokens)\n",
    "    row['Imp Words'] = words\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Unnamed:0'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# df = df.apply(process_row,axis = 1)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUnnamed:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUnnamed: 0.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVerbs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAdjectives\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNouns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Mini Project\\TDD\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:5568\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5421\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5422\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5429\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5432\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5433\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5566\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5567\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5570\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5574\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5575\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5576\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Mini Project\\TDD\\venv\\Lib\\site-packages\\pandas\\core\\generic.py:4782\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4781\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4782\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32md:\\Mini Project\\TDD\\venv\\Lib\\site-packages\\pandas\\core\\generic.py:4824\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4822\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4823\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4824\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4825\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4827\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4828\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Mini Project\\TDD\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7069\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7069\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7070\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7071\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Unnamed:0'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# df = df.apply(process_row,axis = 1)\n",
    "df.drop([\"Unnamed:0\",\"Unnamed: 0.1\",\"Verbs\",\"Adjectives\",\"Nouns\",\"Role\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"dfidf.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt = \"I want to know when Zoonibot should comment on a subject.\"\n",
    "# # txt\n",
    "# t = custom_tokenizer(txt)\n",
    "# l = identify_pos(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'want'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Imp Words'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0.1  Unnamed: 0         Role  \\\n",
      "0                0           0    Data user   \n",
      "1                1           1  UI designer   \n",
      "2                2           2  UI designer   \n",
      "3                3           3  UI designer   \n",
      "4                4           4  UI designer   \n",
      "...            ...         ...          ...   \n",
      "1661          1661        1661        admin   \n",
      "1662          1662        1662        admin   \n",
      "1663          1663        1663        admin   \n",
      "1664          1664        1664        admin   \n",
      "1665          1665        1665        admin   \n",
      "\n",
      "                                             User Story          Verbs  \\\n",
      "0      I want to have the 12-19-2017 deletions proce...  ['processed']   \n",
      "1      I want to redesign the Resources page, so tha...             []   \n",
      "2      I want to report to the Agencies about user t...     ['making']   \n",
      "3      I want to move on to round 2 of DABS or FABS ...    ['landing']   \n",
      "4      I want to move on to round 2 of Homepage edit...             []   \n",
      "...                                                 ...            ...   \n",
      "1661   I want to know when Zoonibot should give an e...             []   \n",
      "1662   I want to know what Zoonibot should say to a ...             []   \n",
      "1663            I want to group subjects by similarity.   ['subjects']   \n",
      "1664   I want to recommend different projects to vol...      ['based']   \n",
      "1665   I want to see a summary of articles, so that ...             []   \n",
      "\n",
      "                     Adjectives  \\\n",
      "0                ['12-19-2017']   \n",
      "1                            []   \n",
      "2           ['aware', 'better']   \n",
      "3                            []   \n",
      "4                            []   \n",
      "...                         ...   \n",
      "1661                         []   \n",
      "1662                         []   \n",
      "1663                         []   \n",
      "1664  ['recommend', 'previous']   \n",
      "1665                ['summary']   \n",
      "\n",
      "                                                  Nouns  \\\n",
      "0                                         ['deletions']   \n",
      "1     ['redesign', 'Resources', 'matches', 'Broker',...   \n",
      "2     ['report', 'Agencies', 'user', 'testing', 'con...   \n",
      "3     ['move', 'DABS', 'FABS', 'edits', 'approvals',...   \n",
      "4          ['move', 'edits', 'approvals', 'leadership']   \n",
      "...                                                 ...   \n",
      "1661                        ['Zoonibot', 'explanation']   \n",
      "1662                          ['Zoonibot', 'volunteer']   \n",
      "1663                            ['group', 'similarity']   \n",
      "1664          ['projects', 'volunteers', 'experiences']   \n",
      "1665                              ['articles', 'reuse']   \n",
      "\n",
      "                                              Imp Words  \\\n",
      "0              [want, 12-19-2017, deletions, processed]   \n",
      "1     [want, redesign, Resources, matches, Broker, d...   \n",
      "2     [want, report, Agencies, user, testing, aware,...   \n",
      "3     [want, move, DABS, FABS, landing, edits, get, ...   \n",
      "4       [want, move, edits, get, approvals, leadership]   \n",
      "...                                                 ...   \n",
      "1661                [want, know, Zoonibot, explanation]   \n",
      "1662                  [want, know, Zoonibot, volunteer]   \n",
      "1663                [want, group, subjects, similarity]   \n",
      "1664  [want, recommend, projects, volunteers, based,...   \n",
      "1665                   [want, summary, articles, reuse]   \n",
      "\n",
      "                                                 Tokens  \\\n",
      "0              [want, 12-19-2017, deletions, processed]   \n",
      "1     [want, redesign, Resources, matches, Broker, d...   \n",
      "2     [want, report, Agencies, user, testing, aware,...   \n",
      "3     [want, move, DABS, FABS, landing, edits, get, ...   \n",
      "4       [want, move, edits, get, approvals, leadership]   \n",
      "...                                                 ...   \n",
      "1661                [want, know, Zoonibot, explanation]   \n",
      "1662                  [want, know, Zoonibot, volunteer]   \n",
      "1663                [want, group, subjects, similarity]   \n",
      "1664  [want, recommend, projects, volunteers, based,...   \n",
      "1665                   [want, summary, articles, reuse]   \n",
      "\n",
      "                                               Text_str  ...  wysiwyg  xhtml  \\\n",
      "0                   want 12-19-2017 deletions processed  ...      0.0    0.0   \n",
      "1     want redesign Resources matches Broker design ...  ...      0.0    0.0   \n",
      "2     want report Agencies user testing aware contri...  ...      0.0    0.0   \n",
      "3     want move DABS FABS landing edits get approval...  ...      0.0    0.0   \n",
      "4              want move edits get approvals leadership  ...      0.0    0.0   \n",
      "...                                                 ...  ...      ...    ...   \n",
      "1661                     want know Zoonibot explanation  ...      0.0    0.0   \n",
      "1662                       want know Zoonibot volunteer  ...      0.0    0.0   \n",
      "1663                     want group subjects similarity  ...      0.0    0.0   \n",
      "1664  want recommend projects volunteers based previ...  ...      0.0    0.0   \n",
      "1665                        want summary articles reuse  ...      0.0    0.0   \n",
      "\n",
      "      zenodo  zero  zeroes  zip  zoning  zoo  zoom  zoonibot  \n",
      "0        0.0   0.0     0.0  0.0     0.0  0.0   0.0  0.000000  \n",
      "1        0.0   0.0     0.0  0.0     0.0  0.0   0.0  0.000000  \n",
      "2        0.0   0.0     0.0  0.0     0.0  0.0   0.0  0.000000  \n",
      "3        0.0   0.0     0.0  0.0     0.0  0.0   0.0  0.000000  \n",
      "4        0.0   0.0     0.0  0.0     0.0  0.0   0.0  0.000000  \n",
      "...      ...   ...     ...  ...     ...  ...   ...       ...  \n",
      "1661     0.0   0.0     0.0  0.0     0.0  0.0   0.0  0.638529  \n",
      "1662     0.0   0.0     0.0  0.0     0.0  0.0   0.0  0.706716  \n",
      "1663     0.0   0.0     0.0  0.0     0.0  0.0   0.0  0.000000  \n",
      "1664     0.0   0.0     0.0  0.0     0.0  0.0   0.0  0.000000  \n",
      "1665     0.0   0.0     0.0  0.0     0.0  0.0   0.0  0.000000  \n",
      "\n",
      "[1666 rows x 2765 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Step 1: Tokenize the text in each row\n",
    "df['Tokens'] = df['Imp Words']\n",
    "\n",
    "# Step 2: Convert the tokenized text into strings\n",
    "df['Text_str'] = df['Tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Step 3: Compute TF-IDF scores for each word in the corpus\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Text_str'])\n",
    "\n",
    "# Step 4: Extract TF-IDF scores for each word\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Step 5: Assign TF-IDF scores to each word in the dataframe\n",
    "df = pd.concat([df,tfidf_df],axis=1)\n",
    "\n",
    "# Display the updated dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1666"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rows.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarities with Existing User Stories:\n",
      "User Story 1: 0.5604726472929079\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sample dataframe with existing user stories and TF-IDF scores\n",
    "# Replace this with your actual dataframe containing TF-IDF scores\n",
    "# Make sure it includes TF-IDF scores for each word\n",
    "data = {\n",
    "    'Text': ['The quick brown fox jumps over the lazy dog.',\n",
    "             'The sun is shining brightly today.',\n",
    "             # Add more user stories as needed\n",
    "            ],\n",
    "    # Example TF-IDF scores for each word in the existing user stories\n",
    "    'want_tfidf': [0.0, 0.0],\n",
    "    'quick_tfidf': [0.0, 0.3],\n",
    "    'brown_tfidf': [0.4, 0.0],\n",
    "    # Add more TF-IDF columns as needed\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# New user story\n",
    "new_user_story = \"A lazy cat is sleeping on the couch.\"\n",
    "\n",
    "# Step 1: Tokenize the new user story\n",
    "new_tokens = word_tokenize(new_user_story)\n",
    "\n",
    "# Step 2: Convert the tokenized new user story into a string\n",
    "new_text_str = ' '.join(new_tokens)\n",
    "\n",
    "# Step 3: Compute TF-IDF scores for the new user story\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([new_text_str] + df['Text'])\n",
    "\n",
    "# Step 4: Calculate the cosine similarity between the TF-IDF vectors\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])[0]\n",
    "\n",
    "# Display the cosine similarities\n",
    "print(\"Cosine Similarities with Existing User Stories:\")\n",
    "for i, similarity in enumerate(cosine_similarities):\n",
    "    print(f\"User Story {i+1}: {similarity}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
