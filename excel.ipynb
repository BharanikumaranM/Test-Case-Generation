{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up Dataset directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = './dataset'\n",
    "data_files = np.array(os.listdir(data_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing out the Userstories in text documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I want to have the 12-19-2017 deletions processed.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = list()\n",
    "for i in os.listdir(data_directory):\n",
    "    path = os.path.join(data_directory,i)\n",
    "    f = open(path,'r')\n",
    "    texts = f.readlines()\n",
    "    for line in texts:\n",
    "        sentences.append(line.strip())\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract User Roles from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'As (?:a|an) ([^,]+)'\n",
    "roles = list()\n",
    "subword_pattern = r'^.*?,'\n",
    "for i in range(0,len(sentences)):\n",
    "    match = re.findall(pattern,sentences[i])\n",
    "    if match:\n",
    "        roles.append(match[0])\n",
    "    else:\n",
    "        roles.append(\"None\")\n",
    "    sentences[i] = re.sub( subword_pattern,'',sentences[i])\n",
    "df['role'] = roles\n",
    "df['user stories'] = sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping out the rows which has no user stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['role']==\"None\"].index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to a Excel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('training.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bOct0\\AppData\\Local\\Temp\\ipykernel_32184\\3348696171.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Verbs</th>\n",
       "      <th>Adjectives</th>\n",
       "      <th>Nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>[(jumps, 4, VBZ)]</td>\n",
       "      <td>[(quick, 1, JJ), (lazy, 7, JJ)]</td>\n",
       "      <td>[(brown, 2, NN), (fox, 3, NN), (dog, 8, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The sun is shining brightly today.</td>\n",
       "      <td>[(is, 2, VBZ), (shining, 3, VBG)]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(sun, 1, NN), (today, 5, NN)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Text  \\\n",
       "0  The quick brown fox jumps over the lazy dog.   \n",
       "1            The sun is shining brightly today.   \n",
       "\n",
       "                               Verbs                       Adjectives  \\\n",
       "0                  [(jumps, 4, VBZ)]  [(quick, 1, JJ), (lazy, 7, JJ)]   \n",
       "1  [(is, 2, VBZ), (shining, 3, VBG)]                               []   \n",
       "\n",
       "                                          Nouns  \n",
       "0  [(brown, 2, NN), (fox, 3, NN), (dog, 8, NN)]  \n",
       "1                [(sun, 1, NN), (today, 5, NN)]  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Sample dataframe with 2 columns and 1700 rows\n",
    "# Replace this with your actual dataframe\n",
    "data = {\n",
    "    'Text': ['The quick brown fox jumps over the lazy dog.',\n",
    "             'The sun is shining brightly today.',\n",
    "             # Add more rows as needed\n",
    "            ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Tag tokens with parts of speech\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    return pos_tags\n",
    "\n",
    "# Function to process each row and append results to dataframe\n",
    "def process_row(row):\n",
    "    text = row['Text']\n",
    "    pos_tags = custom_tokenizer(text)\n",
    "    \n",
    "    # Assign unique IDs to tokens\n",
    "    token_ids = {token: i for i, (token, _) in enumerate(pos_tags)}\n",
    "    \n",
    "    verbs = [(token, token_ids[token], tag) for token, tag in pos_tags if tag.startswith('VB')]\n",
    "    adjectives = [(token, token_ids[token], tag) for token, tag in pos_tags if tag.startswith('JJ')]\n",
    "    nouns = [(token, token_ids[token], tag) for token, tag in pos_tags if tag.startswith('NN')]\n",
    "    \n",
    "    row['Verbs'] = verbs\n",
    "    row['Adjectives'] = adjectives\n",
    "    row['Nouns'] = nouns\n",
    "    return row\n",
    "df = df.apply(process_row, axis=1)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redesign page header and CSS style, with an automatic refresh to ensure there's no overlap with other pages that have been recently modified.\n",
      "\n",
      "See also [ edit ]\n",
      "\n",
      "Notes [ edit ]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2-medium\"  # Change to the appropriate model name if needed\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Example tokens\n",
    "tokens = ['redesign', 'page','header']\n",
    "\n",
    "# Convert tokens to input sequence\n",
    "input_sequence = tokenizer.encode(' '.join(tokens), return_tensors='pt')\n",
    "\n",
    "# Generate text using the model\n",
    "generated_output = model.generate(input_sequence, max_length=50, num_return_sequences=1, do_sample=True)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
