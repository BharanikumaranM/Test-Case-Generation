{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up Dataset directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = './dataset'\n",
    "data_files = np.array(os.listdir(data_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing out the Userstories in text documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I want to have the 12-19-2017 deletions processed.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = list()\n",
    "for i in os.listdir(data_directory):\n",
    "    path = os.path.join(data_directory,i)\n",
    "    f = open(path,'r')\n",
    "    texts = f.readlines()\n",
    "    for line in texts:\n",
    "        sentences.append(line.strip())\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract User Roles from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'As (?:a|an) ([^,]+)'\n",
    "roles = list()\n",
    "subword_pattern = r'^.*?,'\n",
    "for i in range(0,len(sentences)):\n",
    "    match = re.findall(pattern,sentences[i])\n",
    "    if match:\n",
    "        roles.append(match[0])\n",
    "    else:\n",
    "        roles.append(\"None\")\n",
    "    sentences[i] = re.sub( subword_pattern,'',sentences[i])\n",
    "df['role'] = roles\n",
    "df['user stories'] = sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping out the rows which has no user stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['role']==\"None\"].index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to a Excel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('training.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data from the excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('training.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(texts):\n",
    "    texts = texts.lower() # convert to lower case\n",
    "    # Remove punctuation\n",
    "    pattern = r'[^\\w]+'\n",
    "    words = texts.split() #tokenize\n",
    "    for k in range(len(words)):\n",
    "        words[k] = re.sub(pattern, '', words[k])\n",
    "    \n",
    "    texts = ' '.join(words) # Rejoin words with space\n",
    "    texts = re.sub(r'\\s+', ' ', texts) # Remove extra whitespace\n",
    "    \n",
    "    pattern = r'\\b(?:\\d{4}-\\d{2}-\\d{2}|\\d{2}-\\d{2}-\\d{4}|(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s\\d{1,2},?\\s\\d{4}|\\w+)\\b'\n",
    "    match = re.findall(pattern, texts) #find date format\n",
    "    if not match:\n",
    "        texts = re.sub(r'\\d+', '', texts) # Remove numbers\n",
    "        \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = \"\"\n",
    "    words = texts.split()\n",
    "    for word in words:\n",
    "        if word.lower() not in stop_words:\n",
    "            filtered_text += word + ' '   \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading User stories from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['user stories']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "SnowballStem = SnowballStemmer(\"english\")\n",
    "for i in range(0,len(df)):\n",
    "    words = word_tokenize(clean_text(sentences[i]).strip())\n",
    "    stemmed_words = [SnowballStem.stem(word) for word in words]\n",
    "    sentences[i] = ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             want 12192017 delet process\n",
       "1       want redesign resourc page match new broker de...\n",
       "2       want report agenc user test awar contribut mak...\n",
       "3       want move round 2 dab fab land page edit get a...\n",
       "4       want move round 2 homepag edit get approv lead...\n",
       "                              ...                        \n",
       "1663                       want know zoonibot give explan\n",
       "1664                        want know zoonibot say volunt\n",
       "1665                           want group subject similar\n",
       "1666    want recommend differ project volunt base prev...\n",
       "1667                         want see summari articl reus\n",
       "Name: user stories, Length: 1668, dtype: object"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
